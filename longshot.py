'''
Longshot is a model that generates task-specific CLS vectors by running
RNN over GPT-encoded text annotated by span binary dim and attempts to generate
latent dense vector that can be decoded by another LSTM to produce the text
of the question to which the spanned text pertains
'''

# first term is length of word embeddings; second is span dim
EMBEDDING_SIZE = 784 + 1


import torch
import numpy as np
from torch import nn
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Encoder(nn.Module):
    def __init__(self, hiddenDim, layerNum, lr):
        super(Encoder, self).__init__()
        self.hiddenDim = hiddenDim
        self.layerNum = layerNum
        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)
        # TODO: Implement GPT embeddings
        # self.embedding = nn.Embedding(batcherObj.vocabSize, hiddenDim)
        self.rnn = nn.GRU(input_size=hiddenDim,
                          hidden_size=EMBEDDING_SIZE,
                          num_layers=layerNum,
                          batch_first=True)

    def initialize_hidden(self, device):
        """ Init hidden state passed to first RNN cell in time series """
        # initTensor =  torch.zeros(1, 1, self.hiddenDim, device=device)
        return nn.init.xavier_uniform_(initTensor).float()
        return initTensor

    def forward(self, curVec, hidden):
        """
        Forward pass over GPT2 embedding vectors updates rnn cell state for
        later decoding
        """
        # TODO: check shaping of curVec
        curVec = curVec.view(1, 1, -1)
        outSeq, hidden = self.rnn(curVec, hidden)
        return outSeq, hidden


class Decoder(nn.Module):
    def __init__(self, hiddenDim, layerNum, lr):
        # INHERIT
        super(Decoder, self).__init__()
        # PARAMS
        self.hiddenDim = hiddenDim
        self.layerNum = layerNum
        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)
        # LAYERS
        self.rnn = nn.GRU(input_size=hiddenDim,
                          hidden_size=EMBEDDING_SIZE,
                          num_layers=layerNum,
                          batch_first=True)
        self.dense = nn.Linear(in_features=hiddenDim,
                               out_features=batcherObj.vocabSize)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, prevId, hidden):
        """
        Forward pass uses hidden layer generated by encoder and returns tuple
        of softmax categorical vectors across time series and hidden state
        """
        embedOut = self.embedding(prevId).view(1, 1, -1)
        rnnOut, hidden = self.rnn(embedOut, hidden)
        denseOut = self.dense(rnnOut[0])
        outSeq = self.softmax(denseOut)
        return outSeq, hidden


class LongShot(object):
    ''' The LongShot model which aggregates the Encoder and Decoder '''
    def __init__(self):
        self.encoder = Encoder(hiddenDim, layerNum, lr)
        self.decoder = Decoder(hiddenDim, layerNum, lr)

    def categorical_loss(self, predVec, targetId):
        """ Custom loss function to play with """
        predCorrect = predVec[0, targetId]
        predLog = torch.log(predCorrect)
        return -(predLog)

    def eval_accuracy(self, predVec, targetId, teacherForce=True):
        """ Evaluates accuracy of prediciton """
        return 1 if (predVec.max(1)[1] == targetId.max()) else 0

    def train_on_sample(self, contextVecs, span, questionText):
        '''
        Trains model on context/question pair. Runs single pass over vector
        embeddings of context paragraph after adding question-specific
        spannotations (haha). Passes cell state of encoder rnn to decoder rnn
        for character-level question approximation. Evaluated against question
        and backpropped.
        Args:
            contextVecs:        Vectors of GPT-embedded context (no annotations)
            span:               Tuple of span start and end loc for adding
                                    spannotations to contextVecs
            questionText:       String of question text with which to eval model
            teacherForce:       Bool indicating whether to use teacher forcing
                                    in decoder text-generation
        Returns:
            Loss across all decoder predictions on current question
        '''
        # clear optimizer gradients
        self.encoder.optimizer.zero_grad()
        self.decoder.optimizer.zero_grad()
