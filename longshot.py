'''
Longshot is a model that generates task-specific CLS vectors by running
RNN over GPT-encoded text annotated by span binary dim and attempts to generate
latent dense vector that can be decoded by another LSTM to produce the text
of the question to which the spanned text pertains
'''

import torch
import numpy as np
from torch import nn
from tqdm import tqdm, trange
from termcolor import colored
import matplotlib.pyplot as plt
from transformers import GPT2LMHeadModel, GPT2Tokenizer

from stucts import SearchTable

# first term is length of word embeddings; second is span dim
EMBEDDING_SIZE = 784 + 1
# number of dims for categorical outputs (letters, numbers, stopchars, etc)
OUT_SIZE = 26 + 1
# stop token tells the decoder to stop running
STOP_TOKEN = '*'
# maximum number of characters the decoder is allowed to generate per run
DECODER_MAX = 500

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Encoder(nn.Module):
    def __init__(self, hiddenDim, layerNum, lr):
        super(Encoder, self).__init__()
        self.hiddenDim = hiddenDim
        self.layerNum = layerNum
        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)
        self.rnn = nn.GRU(input_size=hiddenDim,
                          hidden_size=EMBEDDING_SIZE,
                          num_layers=layerNum,
                          batch_first=True)

    def initialize_hidden(self, device):
        """ Init hidden state passed to first RNN cell in time series """
        # initTensor =  torch.zeros(1, 1, self.hiddenDim, device=device)
        return nn.init.xavier_uniform_(initTensor).float()
        return initTensor

    def forward(self, curVec, hidden):
        """
        Forward pass over GPT2 embedding vectors updates rnn cell state for
        later decoding
        """
        # TODO: check shaping of curVec
        curVec = curVec.view(1, 1, -1)
        outSeq, hidden = self.rnn(curVec, hidden)
        return outSeq, hidden


class Decoder(nn.Module):
    def __init__(self, hiddenDim, layerNum, lr, outDim):
        '''
        The Decoder Model uses the cell state of the Encoder Model run across
        the word embeddings of the spannotated context to produce step-by-step
        categorical predictions of the character-level encoding of the question
        relating to the current context and span.
        Args:
            hiddenDim:      The size of the hidden vector passed from Encoder
            layerNum:       The number of layers used by the RNN
            lr:             The learning rate of the decoder model
            outDim:         The dimensionality of the character-level space
        '''
        # INHERIT
        super(Decoder, self).__init__()
        # PARAMS
        self.hiddenDim = hiddenDim
        self.layerNum = layerNum
        self.optimizer = torch.optim.Adam(self.encoder.parameters(), lr=lr)
        # LAYERS
        self.rnn = nn.GRU(input_size=hiddenDim,
                          hidden_size=EMBEDDING_SIZE,
                          num_layers=layerNum,
                          batch_first=True)
        self.dense = nn.Linear(in_features=hiddenDim,
                               out_features=batcherObj.vocabSize)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, prevId, hidden):
        """
        Forward pass uses hidden layer generated by encoder and returns tuple
        of softmax categorical vectors across time series and hidden state
        """
        embedOut = self.embedding(prevId).view(1, 1, -1)
        rnnOut, hidden = self.rnn(embedOut, hidden)
        denseOut = self.dense(rnnOut[0])
        outSeq = self.softmax(denseOut)
        return outSeq, hidden


class LongShot(object):
    ''' The LongShot model which aggregates the Encoder and Decoder '''
    def __init__(self, searchTable):
        hiddenDim = searchTable.wordEmbeddingSize + 1
        outDim = searchTable.charEmbeddingSize
        self.encoder = Encoder(hiddenDim, layerNum, lr)
        self.decoder = Decoder(hiddenDim, layerNum, lr)

    def categorical_loss(self, predVec, targetId):
        """ Custom loss function to play with """
        predCorrect = predVec[0, targetId]
        predLog = torch.log(predCorrect)
        return -(predLog)

    def eval_accuracy(self, predVec, targetId, teacherForce=True):
        """ Evaluates accuracy of prediciton """
        return 1 if (predVec.max(1)[1] == targetId.max()) else 0

    def train_step(self, contextVecs, span, questionChars):
        '''
        Trains model on context/question pair. Runs single pass over vector
        embeddings of context paragraph after adding question-specific
        spannotations (haha). Passes cell state of encoder rnn to decoder rnn
        for character-level question approximation. Evaluated against question
        and backpropped.
        Args:
            contextVecs:        Vectors of GPT-embedded context (no annotations)
            span:               Tuple of span start and end loc for adding
                                    spannotations to contextVecs
            questionChars:      Ordered iterable of chars in question
            teacherForce:       Bool indicating whether to use teacher forcing
                                    in decoder text-generation
        Returns:
            Loss across all decoder predictions on current question
        '''
        # clear optimizer gradients
        self.encoder.optimizer.zero_grad()
        self.decoder.optimizer.zero_grad()
        # accumulator for loss and accuracy across data
        loss, numCorrect = 0, 0
        # generate initial hidden state for encoder rnn
        encoderHidden = self.encoder.initialize_hidden(self.device)
        # run encoder across samples
        for encoderStep, wordEmbedding in enumerate(contextVecs):
            (encoderOut,
             encoderHidden) = self.encoder(wordEmbedding, encoderHidden)
             # TODO: decide what to do with encoder outs
            _ = encoderOut[0, 0]
        # TODO: Get targets by running splitting function from object methods
        targets = [c for c in questionText]
        targetLen = len(targets)
        # TODO: Get embedding of start char to kick-off decoder
        decoderInput = None
        # initial decoder hidden state is final encoder hidden state
        # REVIEW: does encoder hidden need to be saved
        decoderHidden = encoderHidden
        # run decoder across encoderOuts, initializing with encoderHidden
        for decoderStep in range(DECODER_MAX):
            (decoderOut,
             decoderHidden) = self.decoder(decoderInput, decoderHidden)
            # fetch most recent decoder pred for next step input
            _, topi = decoderOut.topk(1)
            decoderInput = topi.squeeze().detach()
            # find what the decoder is supposed to ouput
            if (decoderStep <= targetLen):
                curTarget = targets[decoderStep]
            else:
                curTarget = None
            # update loss and check if decoder has ouput END char
            loss += self.categorical_loss(decoderOut, targets[decoderStep])
            numCorrect += self.eval_accuracy(decoderOut, targets[decoderStep])
            if decoderInput.item() == 'STOP_CHAR_NUM_TO_DO':
                break
        return (loss.item() / decoderStep), (numCorrect / decoderStep))

        def train(self, searchTable, epochs, plot=False):
            '''
            Trains both encoder and decoder on SearchTable for iterations.
            Uses only questions with answers. Handles all GPT and character
            embeddings. SearchTable is immediately ready for training after
            initialization.
            Args:
                searchTable:    Initialized SearchTable object to train on
                epochs:         Number of passes to make over ALL data in table
                plot (opt):     Whether to generate plots of training progress
            Returns:
                Tuple of form (trained_encoder, trained_decoder)
            '''
            # import and initialize gpt models for embedding
            print(colored('Loading GPT2 Models', 'red'), end='\r')
            gptModel = GPT2LMHeadModel.from_pretrained('gpt2')
            gptTokenizer = GPT2Tokenizer.from_pretrained('gpt2')
            model.eval()
            print(colored('Complete: Loading GPT2 Models', 'cyan'))

            print(colored('Initalizing Methods', 'red'), end='\r')
            # initialize vecs to store loss over time
            lossVec, accVec, testLossVec, testAccVec = [], [], [], []

            # initialize methods for text modification
            def embed_text(text):
                '''
                Helper embeds cleaned text with GPT tokenizer and model
                Returns numpy array of shape (seqLen, EMBEDDING_SIZE)
                '''
                # WARNING: DOES NOT YET WORK
                wordIds = gptTokenizer.encode(text, add_special_tokens=False)
                context = torch.tensor(wordIds, dtype=torch.long, device=device)
                context = context.unsqueeze(0).repeat(1, 1)
                with torch.no_grad():
                    for _ in trange(length, leave=False):
                        inputs = {'input_ids': generated}
                        outputs = model(**inputs)
                # TODO: reshape outputs and pull only embeddings
                return None

            def char_tokenize(text):
                ''' Tokenizes text at character level. Used for questions '''
                # TODO: Implement
                pass
            print(colored('Complete: Initalizing Methods', 'red'), end='\r')

            print(colored(f'Training for {epochs}', 'red'), end='\r')
            # train over data for epochs
            for epoch in trange(epochs):
                for doc in searchTable.iter_docs():
                    docEmbeddings = embed_text(doc.text)
                    for question, span in doc.iter_questions():
                        if not span:
                            break
                        questionChars = char_tokenize(question)
                        # TODO: Get proper span selection and append to docEmbeddings
                        loss, acc = self.train_step(contextVecs, questionChars)
